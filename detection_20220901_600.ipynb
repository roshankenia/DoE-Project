{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import torch\nimport os\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom torchvision import datasets, transforms, models\nfrom PIL import Image\nfrom xml.dom.minidom import parse\nfrom engine import train_one_epoch, evaluate\nimport utils\nimport transforms as T\nfrom engine import train_one_epoch, evaluate\n# utils、transforms、engine are the utils.py、transforms.py、engine.py under this fold\n\n%matplotlib inline",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "9b5d142a"
    },
    {
      "cell_type": "code",
      "source": "def normalize(arr):\n    \"\"\"\n    Linear normalization\n    normalize the input array value into [0, 1]\n    http://en.wikipedia.org/wiki/Normalization_%28image_processing%29\n    \"\"\"\n    arr = arr.astype('float')\n    #print(\"...arr shape\", arr.shape)\n    #print(\"arr shape: \", arr.shape)\n    # Do not touch the alpha channel\n    for i in range(3):\n        minval = arr[i,:,:].min()\n        maxval = arr[i,:,:].max()\n        if minval != maxval:\n            arr[i,:,:] -= minval\n            arr[i,:,:] /= (maxval-minval)\n    return arr",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "89c7590c"
    },
    {
      "cell_type": "code",
      "source": "# define our own dataset\nclass PebbleDataset(torch.utils.data.Dataset):\n    def __init__(self, root, transforms=None):\n        self.root = root\n        self.transforms = transforms\n        # load all image and annotation files, sorting them to ensure that the images and their annotations are aligned\n        self.imgs = list(sorted(os.listdir(os.path.join(root, \"JPEGImages\"))))\n        self.bbox_xml = list(sorted(os.listdir(os.path.join(root, \"Annotations\"))))\n \n    def __getitem__(self, idx):\n        # load images and their corresponding bbox annotations\n        img_path = os.path.join(self.root, \"JPEGImages\", self.imgs[idx])\n        bbox_xml_path = os.path.join(self.root, \"Annotations\", self.bbox_xml[idx])\n        img = Image.open(img_path).convert(\"RGB\")\n        # normalization\n        #img = np.array(img)\n        #img = normalize(img)\n        #img = Image.fromarray(normalize(img),'RGB')\n        \n        \n        # read the annotation files from the path, which are in xml format\n        dom = parse(bbox_xml_path)\n        # get the element of the annotation files\n        data = dom.documentElement\n        # get the objects of the elements\n        objects = data.getElementsByTagName('object')        \n        # extract the content of the annotation file, which includes class label and bounding box coordinates\n        boxes = []\n        labels = []\n        for object_ in objects:\n            # extract the class label, 0 for background, 1 and 2 for mark_type_1 and mark_type_2 respectively\n            name = object_.getElementsByTagName('name')[0].childNodes[0].nodeValue\n            #labels.append(np.int(name[-1]))\n            labels.append(np.int(name.split(\"_\")[-1]))\n\n            # extract the bounding box coordinates\n            bndbox = object_.getElementsByTagName('bndbox')[0]\n            xmin = np.float(bndbox.getElementsByTagName('xmin')[0].childNodes[0].nodeValue)\n            ymin = np.float(bndbox.getElementsByTagName('ymin')[0].childNodes[0].nodeValue)\n            xmax = np.float(bndbox.getElementsByTagName('xmax')[0].childNodes[0].nodeValue)\n            ymax = np.float(bndbox.getElementsByTagName('ymax')[0].childNodes[0].nodeValue)\n            boxes.append([xmin, ymin, xmax, ymax])        \n \n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        # there is only one class\n        labels = torch.as_tensor(labels, dtype=torch.int64)        \n \n        image_id = torch.tensor([idx])\n        # calculate the area of the bounding box of each instance\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((len(objects),), dtype=torch.int64)\n \n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n \n        if self.transforms is not None:\n            # https://github.com/pytorch/vision/tree/master/references/detection  \n            # On this website, there are transform examples of RandomHorizontalFlip for target in transforms.py\n            img, target = self.transforms(img, target)\n \n        return img, target\n \n    def __len__(self):\n        return len(self.imgs)",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "9f9436a3"
    },
    {
      "cell_type": "code",
      "source": "def get_transform(train):\n    transf = []\n    # convert the image, a PIL image, into a PyTorch Tensor\n    transf.append(T.PILToTensor())\n    if train:\n        # during training, randomly flip the training images\n        # and ground-truth with 50% probability for data augmentation\n        transf.append(T.RandomHorizontalFlip(0.5))\n \n    return T.Compose(transf)",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "8a59976c"
    },
    {
      "cell_type": "markdown",
      "source": "### start to train the model",
      "metadata": {},
      "id": "ff3e7fe8"
    },
    {
      "cell_type": "code",
      "source": "root = r'/data/home/stufs1/zuwang/DoE_Project/new_datasets_annotations/doe_dataset_GT_white_ink_voc_20220901_600'\n# train on the GPU (specify GPU ID with 'cuda:id'), or on the CPU if a GPU is not available\ndevice = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n# 11 classes, 0, 1，..., 9, and background\nnum_classes = 11\n# use our dataset with defined transformations\n# note that the 'dataset' and 'dataset_test' are with the same images in the same order\n# the only difference is that images in 'dataset' can be randomly flipped,\n# while images in 'dataset_test' are not flipped\ndataset = PebbleDataset(root, get_transform(train=True))\ndataset_test = PebbleDataset(root, get_transform(train=False))\n\n# split the dataset (399 images in total) into\n# training set (300 images) and test set (99 images)\ns_ratio = 300\nindices = torch.randperm(len(dataset)).tolist()\ndataset = torch.utils.data.Subset(dataset, indices[:s_ratio])\ndataset_test = torch.utils.data.Subset(dataset_test, indices[s_ratio:])\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=2, shuffle=True, # num_workers=4,\n    collate_fn=utils.collate_fn)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test, batch_size=2, shuffle=False, # num_workers=4,\n    collate_fn=utils.collate_fn)\n\n# get the model using our helper function\nmodel = models.detection.fasterrcnn_resnet50_fpn(pretrained=False, progress=True, num_classes=num_classes, pretrained_backbone=True)  # or get_object_detection_model(num_classes)\n\n# move model to the right device\nmodel.to(device)\n\n# collect the trainable parameters in the model\nparams = [p for p in model.parameters() if p.requires_grad]\n\n# define the optimizer, here we use SGD\noptimizer = torch.optim.SGD(params, lr=0.0003, momentum=0.9, weight_decay=0.0005)\n\n# define a learning rate scheduler\nlr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2)\n\n# let's train it for a defined number of epochs\nnum_epochs = 100\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n        # images and targets are all put '.to(device)' by the 'train_one_epoch' in engine.py\n    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=50)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset    \n    evaluate(model, data_loader_test, device=device)    \n    \n    print('')\n    print('==================================================')\n    print('')\nprint(\"Training is done!\")",
      "metadata": {
        "scrolled": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "eca20873"
    },
    {
      "cell_type": "code",
      "source": "# save the trained model\ntorch.save(model, r'./saved_model/model_doe_20220901_600_multi_class_100epochs.pkl')",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "d2344030"
    },
    {
      "cell_type": "code",
      "source": "def showbbox(model, img, idx):\n    # the input images are tensors with values in [0, 1]\n    #print(\"input image shape...:\", type(img))\n    image_array = img.numpy()\n    image_array = np.array(normalize(image_array),dtype=np.float32) \n    img = torch.from_numpy(image_array)\n    \n    model.eval()\n    with torch.no_grad():\n        '''\n        prediction is in the following format：\n        [{'boxes': tensor([[1492.6672,  238.4670, 1765.5385,  315.0320],\n        [ 887.1390,  256.8106, 1154.6687,  330.2953]], device='cuda:0'), \n        'labels': tensor([1, 1], device='cuda:0'), \n        'scores': tensor([1.0000, 1.0000], device='cuda:0')}]\n        '''\n        \n        prediction = model([img.to(device)])\n        \n    print(prediction)\n        \n    img = img.permute(1,2,0)  # C,H,W → H,W,C\n    img = (img * 255).byte().data.cpu()  # [0, 1] → [0, 255]\n    img = np.array(img)  # tensor → ndarray\n       \n    # for i in range(prediction[0]['boxes'].cpu().shape[0]): # select all the predicted bounding boxes\n    for i in range(3): # select the top-3 predicted bounding boxes\n        xmin = round(prediction[0]['boxes'][i][0].item())\n        ymin = round(prediction[0]['boxes'][i][1].item())\n        xmax = round(prediction[0]['boxes'][i][2].item())\n        ymax = round(prediction[0]['boxes'][i][3].item())\n        \n        label = prediction[0]['labels'][i].item()\n        \n        if label == 10: # start with background as 0\n            cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (255, 0, 0), thickness=1)\n            cv2.putText(img, '0', (xmin, ymin), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), thickness=2)\n        elif label == 1:\n            cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 255, 0), thickness=1)\n            cv2.putText(img, '1', (xmin, ymin), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), thickness=2)   \n        elif label == 2:\n            cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 0, 255), thickness=1)\n            cv2.putText(img, '2', (xmin, ymin), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), thickness=2)\n        elif label == 3:\n            cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 100, 255), thickness=1)\n            cv2.putText(img, '3', (xmin, ymin), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 100, 255), thickness=2)\n        elif label == 4:\n            cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (255, 100, 100), thickness=1)\n            cv2.putText(img, '4', (xmin, ymin), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 100, 100), thickness=2)\n        elif label == 5:\n            cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (255, 0, 255), thickness=1)\n            cv2.putText(img, '5', (xmin, ymin), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 255), thickness=2)\n        elif label == 6:\n            cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 255, 255), thickness=1)\n            cv2.putText(img, '6', (xmin, ymin), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), thickness=2)\n        elif label == 7:\n            cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (255, 255, 0), thickness=1)\n            cv2.putText(img, '7', (xmin, ymin), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), thickness=2)\n        elif label == 8:\n            cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (100, 0, 0), thickness=1)\n            cv2.putText(img, '8', (xmin, ymin), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 0, 0), thickness=2)\n        elif label == 9:\n            cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 0, 100), thickness=1)\n            cv2.putText(img, '9', (xmin, ymin), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 100), thickness=2)            \n    \n    # plot the output images with prediction result\n    plt.figure(figsize=(50,50))\n    plt.imshow(img)\n    plt.axis('off')\n    vis_tgt_path = \"./visualization_results/\"\n    if not os.path.isdir(vis_tgt_path):\n        os.mkdir(vis_tgt_path)\n    plt.savefig(os.path.join(vis_tgt_path, \"sample_\" + f\"{idx:02d}\" + \"_vis.png\"))",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "9edd0646"
    },
    {
      "cell_type": "code",
      "source": "# check the result\nmodel = torch.load(r'./saved_model/model_doe_20220901_600_multi_class_100epochs.pkl')\ndevice = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)\n\nfor idx in range(len(dataset_test)):\n    img, _ = dataset_test[idx] \n    showbbox(model, img, idx)",
      "metadata": {
        "scrolled": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "3e63dc34"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "60af27d1"
    }
  ]
}